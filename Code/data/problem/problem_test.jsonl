{"problem_name": "Lasso problem", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|Ax-b\\|^2 + \\mu \\|x\\|_1.", "class": "proximal gradient", "algorithm": "Let \\( f(x) = \\frac{1}{2} \\|Ax - b\\|^2 \\), \\( h(x) = \\mu \\|x\\|_1 \\). Then,\\[ \n\\nabla f(x) = A^{\\top}(Ax - b), \\quad \n\\operatorname{prox}_{t_k h}(x) = \\operatorname{sign}(x)\\max\\{|x| - t_k \\mu, 0\\}. \n\\] \nTo solve the LASSO problem, the proximal gradient method can be expressed in the following iterative form: \n\\[ \ny^k = x^k - t_k A^{\\top}(Ax^k - b), \n\\] \n\\[ \nx^{k+1} = \\operatorname{sign}(y^k)\\max\\{|y^k| - t_k \\mu, 0\\}. \n\\]"}
{"problem_name": "wavelet decomposition model", "problem_statement": "\\min_{d \\in \\mathbb{R}^n} \\|\\lambda \\odot d\\|_1 + \\frac{1}{2}\\|A W^{T} d - b\\|^2", "class": "proximal gradient", "algorithm": "Let \\(f(d)=\\frac{1}{2}\\|A W^{T} d - b\\|^2\\), \\(h(d)=\\|\\lambda \\odot d\\|_1\\). Then\\[\n\\nabla f(d)=W A^{T}(A W^{T} d - b),\n\\qquad\n\\operatorname{prox}_{t h}(d)=\\operatorname{sign}(d)\\odot\\max\\bigl\\{|d| - t\\lambda,0\\bigr\\}.\n\\]\nTo solve the wavelet decomposition model by proximal gradient, we iterate\n\\[\ny^{k}=d^{k}-t_{k}W A^{T}(A W^{T} d^{k}-b),\n\\]\n\\[\nd^{k+1}=\\operatorname{sign}(y^{k})\\odot\\max\\bigl\\{|y^{k}|-t_{k}\\lambda,0\\bigr\\},\n\\]\nwith step size \\(t_{k}=1/L\\), where \\(L=\\|A W^{T}\\|^2\\) is the Lipschitz constant of \\(\\nabla f\\)."}
{"problem_name": "Balanced wavelet model", "problem_statement": "\\min_{\\alpha \\in \\mathbb{R}^n}  \\|\\lambda \\odot \\alpha\\|_1 + \\frac{\\kappa}{2}\\bigl\\|(I - W W^T)\\alpha\\bigr\\|_2^2 + \\frac{1}{2}\\bigl\\|A W^T\\alpha - b\\bigr\\|_2^2, where (WW^T)^2 = WW^T.", "class": "proximal gradient", "algorithm": "Let \\(f(\\alpha)=\\frac{\\kappa}{2}\\|(I-W W^T)\\alpha\\|_2^2 + \\frac{1}{2}\\|AW^T\\alpha - b\\|_2^2\\), \\(h(\\alpha)=\\|\\lambda\\odot\\alpha\\|_1\\).  Then\n\\[\n\\nabla f(\\alpha)=\\kappa(I-W W^T)\\alpha + W A^T(AW^T\\alpha - b),\n\\quad\n\\operatorname{prox}_{t h}(\\alpha)=\\operatorname{sign}(\\alpha)\\odot\\max(|\\alpha|-t\\lambda,0).\n\\]\nWith step size \\(t=1/L\\), where \\(L=\\kappa\\|I-W W^T\\|^2+\\|AW^T\\|^2\\), the proximal‐gradient updates are\n\\[\ny^k=\\alpha^k - t\\bigl(\\kappa(I-W W^T)\\alpha^k + W A^T(AW^T\\alpha^k - b)\\bigr),\n\\]\n\\[\n\\alpha^{k+1}=\\operatorname{sign}(y^k)\\odot\\max(|y^k|-t\\lambda,0).\n\\]"}
{"problem_name": "Sparse logistic regression", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^m \\log\\bigl(1+e^{-b_i a_i^T x}\\bigr) + \\lambda\\|x\\|_1", "class": "proximal gradient", "algorithm": "Let \\(f(x)=\\sum_{i=1}^m\\log(1+e^{-b_i a_i^T x})\\), \\(h(x)=\\lambda\\|x\\|_1\\). Then\n\\[\n\n\\nabla f(x)= -\\sum_{i=1}^m \\frac{b_i}{1+e^{b_i a_i^T x}}a_i,\n\\qquad\n\\operatorname{prox}_{t h}(u)=\\operatorname{sign}(u)\\odot\\max(|u|-t\\lambda,0).\n\\]\nWith step size \\(t=1/L\\) where \\(L=\\frac14\\|A\\|^2\\), the updates are\n\\[\ny^{k}=x^{k} - t\\nabla f(x^{k}),\n\\]\n\\[\nx^{k+1}=\\operatorname{prox}_{t\\lambda\\|\\cdot\\|_1}(y^{k}).\n\\]"}
{"problem_name": "Group Lasso", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda\\sum_{g=1}^G\\|x_{G_g}\\|_2", "class": "proximal gradient", "algorithm": "Let \\(f(x)=\\frac{1}{2}\\|Ax-b\\|_2^2\\), \\(h(x)=\\lambda\\sum_{g=1}^G\\|x_{G_g}\\|_2\\). Then\n\\[\n\n\\nabla f(x)=A^T(Ax-b),\n\\qquad\n\\operatorname{prox}_{t h}(u)_{G_g}=\\bigl(1-\\frac{t\\lambda}{\\|u_{G_g}\\|_2}\\bigr)_{+}u_{G_g}.\n\\]\nWith \\(t=1/L\\) (\\(L=\\|A\\|^2\\)), iterate\n\\[\ny^{k}=x^{k}-tA^T(Ax^{k}-b),\n\\]\n\\[\nx^{k+1}=\\operatorname{prox}_{t\\lambda\\sum\\|\\cdot\\|_2}(y^{k}),\n\\]\napplying the above group‐wise shrinkage."}
{"problem_name": "Generalized elastic net", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|B x\\|_2^2", "class": "proximal gradient", "algorithm": "Let \\(f(x)=\\|Ax-b\\|_2^2 + \\lambda_2\\|B x\\|_2^2\\), \\(h(x)=\\lambda_1\\|x\\|_1\\). Then\n\\[\n\n\\nabla f(x)=2A^T(Ax-b) + 2\\lambda_2 B^T(Bx),\n\\qquad\n\\operatorname{prox}_{t h}(u)=\\operatorname{sign}(u)\\odot\\max(|u|-t\\lambda_1,0).\n\\]\nWith step size \\(t=1/L\\), \\(L=2\\|A\\|^2+2\\lambda_2\\|B\\|^2\\), update\n\\[\ny^{k}=x^{k} - t\\bigl(2A^T(Ax^{k}-b)+2\\lambda_2 B^T(Bx^{k})\\bigr),\n\\]\n\\[\nx^{k+1}=\\operatorname{prox}_{t\\lambda_1\\|\\cdot\\|_1}(y^{k}).\n\\]"}
{"problem_name": "Elastic Net regression", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\frac{\\lambda_2}{2}\\|x\\|_2^2", "class": "proximal gradient", "algorithm": "Let \\(f(x)=\\frac{1}{2}\\|Ax-b\\|_2^2 + \\frac{\\lambda_2}{2}\\|x\\|_2^2\\), \\(h(x)=\\lambda_1\\|x\\|_1\\). Then\n\\[\n\n\\nabla f(x)=A^T(Ax-b)+\\lambda_2 x,\n\\qquad\n\\operatorname{prox}_{t h}(u)=\\operatorname{sign}(u)\\odot\\max(|u|-t\\lambda_1,0).\n\\]\nWith step size \\(t=1/L\\), \\(L=\\|A\\|^2+\\lambda_2\\), the updates are\n\\[\ny^{k}=x^{k} - t\\bigl(A^T(Ax^{k}-b)+\\lambda_2 x^{k}\\bigr),\n\\]\n\\[\nx^{k+1}=\\operatorname{prox}_{t\\lambda_1\\|\\cdot\\|_1}(y^{k}).\n\\]"}
{"problem_name": "L2-regularized least squares", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|Ax - b\\|^2 + \\mu \\|x\\|_2.", "class": "proximal gradient", "algorithm": "Let \\( f(x) = \\frac{1}{2} \\|Ax - b\\|^2 \\), \\( h(x) = \\mu \\|x\\|_2 \\). Then,\\[ \n\\nabla f(x) = A^{\\top}(Ax - b), \\quad \n\\operatorname{prox}_{t_k h}(x) = \\begin{cases} 0, & \\text{if } \\|x\\|_2 \\leq t_k \\mu \\\\ \\left(1 - \\frac{t_k \\mu}{\\|x\\|_2}\\right) x, & \\text{otherwise} \\end{cases} \n\\] \nTo solve the L2-regularized problem, the proximal gradient method can be expressed as: \n\\[ \ny^k = x^k - t_k A^{\\top}(Ax^k - b), \n\\] \n\\[ \nx^{k+1} = \\begin{cases} 0, & \\text{if } \\|y^k\\|_2 \\leq t_k \\mu \\\\ \\left(1 - \\frac{t_k \\mu}{\\|y^k\\|_2}\\right) y^k, & \\text{otherwise} \\end{cases} \n\\]"}
{"problem_name": "Robust Regression (Huber + L1)", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^m \\phi_\\delta(a_i^T x - b_i) + \\lambda\\|x\\|_1, where the Huber loss \\(\\phi_\\delta(r)\\) is defined as\\[\\phi_\\delta(r) = \\begin{cases} \\frac{1}{2}r^2, & \\text{if } |r| \\leq \\delta \\\\ \\delta(|r| - \\frac{1}{2}\\delta), & \\text{if } |r| > \\delta \\end{cases}\\]", "class": "proximal gradient", "algorithm": "Let \\(f(x) = \\sum_{i=1}^m \\phi_\\delta(a_i^T x - b_i)\\). Then the gradient is\\[\\nabla f(x) = \\sum_{i=1}^m z(x) a_i, \\quad \\text{where } z_i(x) = \\begin{cases} a_i^T x - b_i, & \\text{if } |a_i^T x - b_i| \\leq \\delta \\\\ \\delta \\cdot \\operatorname{sign}(a_i^T x - b_i), & \\text{otherwise} \\end{cases}\\]The proximal operator of \\(h(x) = \\lambda\\|x\\|_1\\) is:\\[\\operatorname{prox}_{t h}(u) = \\operatorname{sign}(u) \\odot \\max(|u| - t\\lambda, 0)\\]With step size \\(t = 1/L\\), the proximal gradient iterations are:\\[y^k = x^k - t \\nabla f(x^k),\\]\\[x^{k+1} = \\operatorname{prox}_{t\\lambda\\|\\cdot\\|_1}(y^k)\\]"}
{"problem_name": "Least squares problem", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|Ax - b\\|_2^2.", "class": "gradient descent", "algorithm": "Let \\( f(x) = \\frac{1}{2} \\|Ax - b\\|^2 \\). The gradient is \\[ \\nabla f(x) = A^{\\top}(Ax - b). \\] The gradient descent update rule is: \\[ x^{k+1} = x^k - t_k A^{\\top}(Ax^k - b). \\]"}
{"problem_name": "Ridge regression", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|Ax - b\\|_2^2 + \\frac{\\lambda}{2} \\|x\\|_2^2.", "class": "gradient descent", "algorithm": "Let \\( f(x) = \\frac{1}{2} \\|Ax - b\\|^2 + \\frac{\\lambda}{2} \\|x\\|^2 \\). The gradient is \\[ \\nabla f(x) = A^{\\top}(Ax - b) + \\lambda x. \\] The gradient descent update is: \\[ x^{k+1} = x^k - t_k (A^{\\top}(Ax^k - b) + \\lambda x^k). \\]"}
{"problem_name": "Logistic regression", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^m \\log(1 + e^{-b_i a_i^{\\top} x})", "class": "gradient descent", "algorithm": "Let \\( f(x) = \\sum_{i=1}^m \\log(1 + e^{-b_i a_i^{\\top} x}) \\). The gradient is \\[ \\nabla f(x) = -\\sum_{i=1}^m \\frac{b_i a_i}{1 + e^{b_i a_i^{\\top} x}}. \\] The gradient descent update rule is: \\[ x^{k+1} = x^k + t_k \\sum_{i=1}^m \\frac{b_i a_i}{1 + e^{b_i a_i^{\\top} x^k}}. \\]"}
{"problem_name": "Quadratic minimization", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} x^{\\top} Q x + c^{\\top} x, where \\( Q \\succ 0 \\) (positive definite).", "class": "gradient descent", "algorithm": "Let f(x) = x^{\\top} Q x + c^{\\top} x, \\[ \\nabla f(x) = Qx + c. \\] The gradient descent update rule is: \\[ x^{k+1} = x^k - t_k(Qx^k + c). \\]"}
{"problem_name": "Tikhonov regularization", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|Ax - b\\|_2^2 + \\frac{\\lambda}{2} \\|Lx\\|_2^2", "class": "gradient descent", "algorithm": "Let \\( f(x) = \\frac{1}{2} \\|Ax - b\\|^2 + \\frac{\\lambda}{2} \\|Lx\\|^2 \\). The gradient is: \\[ \\nabla f(x) = A^\\top(Ax - b) + \\lambda L^\\top L x. \\] The gradient descent update is: \\[ x^{k+1} = x^k - t_k \\nabla f(x^k). \\]"}
{"problem_name": "Binary classification with exponential loss", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^m e^{-b_i a_i^\\top x} + \\frac{\\lambda}{2} \\|x\\|^2", "class": "gradient descent", "algorithm": "Let \\( f(x) = \\sum_{i=1}^m e^{-b_i a_i^\\top x} + \\frac{\\lambda}{2} \\|x\\|^2 \\). Then the gradient is \\[ \\nabla f(x) = -\\sum_{i=1}^m b_i a_i e^{-b_i a_i^\\top x} + \\lambda x. \\] Gradient descent update: \\[ x^{k+1} = x^k - t_k \\nabla f(x^k) \\]."}
{"problem_name": "Audio denoising with harmonic regularization", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\|x - y\\|_2^2 + \\lambda \\|H x\\|_2^2", "class": "gradient descent", "algorithm": "Let \\( f(x) = \\|x - y\\|^2 + \\lambda \\|Hx\\|^2 \\). The gradient is \\[ \\nabla f(x) = 2(x - y) + 2\\lambda H^\\top H x. \\] The gradient descent update rule is: \\[ x^{k+1} = x^k - t_k \\nabla f(x^k). \\]"}
{"problem_name": "Logistic regression with L2 penalty", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^m \\log(1 + e^{-b_i a_i^{\\top} x}) + \\frac{\\lambda}{2} \\|x\\|_2^2", "class": "gradient descent", "algorithm": "Let \\( f(x) = \\sum_{i=1}^m \\log(1 + e^{-b_i a_i^{\\top} x}) + \\frac{\\lambda}{2} \\|x\\|^2 \\). The gradient is \\[ \\nabla f(x) = -\\sum_{i=1}^m \\frac{b_i a_i}{1 + e^{b_i a_i^{\\top} x}} + \\lambda x. \\] The gradient descent update rule is: \\[ x^{k+1} = x^k - t_k \\left( -\\sum_{i=1}^m \\frac{b_i a_i}{1 + e^{b_i a_i^{\\top} x^k}} + \\lambda x^k \\right). \\]"}
{"problem_name": "Smooth image denoising with first-order regularization", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|x - y\\|^2 + \\lambda ( \\|D_1 x\\|^2 + \\|D_2 x\\|^2 ), where D_1 and D_2 \\in \\mathbb{R}^{m \\times n}", "class": "gradient descent", "algorithm": "Let \\( f(x) = \\frac{1}{2} \\|x - y\\|^2 + \\lambda ( \\|D_1 x\\|^2 + \\|D_2 x\\|^2 ) \\), where \\( D_1, D_2 \\) are discrete gradient operators along rows and columns. The gradient is given by \\[ \\nabla f(x) = x - y + 2\\lambda (D_1^\\top D_1 x + D_2^\\top D_2 x). \\] The gradient descent update is: \\[ x^{k+1} = x^k - t_k \\nabla f(x^k). \\]"}
{"problem_name": "Lasso problem", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|Ax-b\\|^2 + \\mu \\|x\\|_1.", "class": "Nesterov", "algorithm": "Let \\( f(x) = \\frac{1}{2} \\|Ax - b\\|^2 \\), so \\( \\nabla f(x) = A^\\top (Ax - b) \\). Let \\( h(x) = \\mu \\|x\\|_1 \\). Use Nesterov's acceleration with fixed step size \\( t_k = \\frac{1}{L} \\), where \\( L > 0 \\) is the Lipschitz constant of \\( \\nabla f \\). Momentum parameters are \\( \\gamma_k = \\frac{2}{k+1} \\) for \\( k \\geq 1 \\), and \\( \\gamma_0 = \\frac{1}{2} \\). The iteration is:\n\\[ z^k = (1 - \\gamma_k) x^{k-1} + \\gamma_k y^{k-1}, \\]\n\\[ y^k = \\operatorname{SoftThreshold}\\left(y^{k-1} - \\frac{t_k}{\\gamma_k} A^\\top (A z^k - b), \\frac{t_k}{\\gamma_k} \\mu \\right), \\]\n\\[ x^k = (1 - \\gamma_k) x^{k-1} + \\gamma_k y^k. \\]\nThe soft-thresholding operator is defined component-wise as:\n\\[ \\operatorname{SoftThreshold}(v, \\tau)_i = \\operatorname{sign}(v_i) \\cdot \\max(|v_i| - \\tau, 0). \\]"}
{"problem_name": "wavelet decomposition model", "problem_statement": "\\min_{d \\in \\mathbb{R}^n} \\|\\lambda \\odot d\\|_1 + \\frac{1}{2}\\|A W^{T} d - b\\|^2", "class": "Nesterov", "algorithm": "Use Nesterov acceleration with fixed \\(t_k = 1/L\\), where \\(L = \\|A W^T\\|^2\\) is the Lipschitz constant of \\(\\nabla f\\). Define:\n\\[\nz^{k}=(1-\\gamma_{k})d^{k-1}+\\gamma_{k}y^{k-1},\n\\]\n\\[\ny^{k}=\\operatorname{SoftThreshold}\\left(y^{k-1}-\\frac{t_{k}}{\\gamma_{k}}\\nabla f(z^{k}), \\frac{t_k}{\\gamma_k} \\lambda\\right),\n\\]\n\\[\nd^{k}=(1-\\gamma_{k})d^{k-1}+\\gamma_{k}y^{k}.\n\\]\nSoftThreshold operator: \\[ \\operatorname{SoftThreshold}(v, \\tau) = \\operatorname{sign}(v) \\cdot \\max(|v| - \\tau, 0). \\]"}
{"problem_name": "Balanced wavelet model", "problem_statement": "\\min_{\\alpha \\in \\mathbb{R}^n}  \\|\\lambda \\odot \\alpha\\|_1 + \\frac{\\kappa}{2}\\bigl\\|(I - W W^T)\\alpha\\bigr\\|_2^2 + \\frac{1}{2}\\bigl\\|A W^T\\alpha - b\\bigr\\|_2^2", "class": "Nesterov", "algorithm": "Use Nesterov acceleration with fixed \\(t = 1/L\\), where \\(L = \\|A W^T\\|^2 + \\kappa\\|I - WW^T\\|^2\\).\n\\[\nz^k=(1-\\gamma_k)\\alpha^{k-1}+\\gamma_k y^{k-1},\n\\]\n\\[\ny^k=\\operatorname{SoftThreshold}\\left(z^k-\\frac{t}{\\gamma_k}\\nabla f(z^k), \\frac{t}{\\gamma_k}\\lambda\\right),\n\\]\n\\[\n\\alpha^k=(1-\\gamma_k)\\alpha^{k-1}+\\gamma_k y^k\n\\]\nSoftThreshold operator: \\[ \\operatorname{SoftThreshold}(v, \\tau) = \\operatorname{sign}(v) \\cdot \\max(|v| - \\tau, 0). \\]"}
{"problem_name": "Sparse logistic regression", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^m \\log\\bigl(1+e^{-b_i a_i^T x}\\bigr) + \\lambda\\|x\\|_1", "class": "Nesterov", "algorithm": "Use Nesterov acceleration with fixed \\(t = 1/L\\), where \\(L = \\frac{1}{4}\\|A\\|^2\\) if \\(\\|a_i\\| \\leq 1\\). Define:\n\\[\nz^{k}=(1-\\gamma_k)x^{k-1}+\\gamma_k y^{k-1},\n\\]\n\\[\ny^{k}=\\operatorname{SoftThreshold}\\left(z^{k}-\\frac{t}{\\gamma_k}\\nabla f(z^{k}), \\frac{t}{\\gamma_k}\\lambda\\right),\n\\]\n\\[\nx^{k}=(1-\\gamma_k)x^{k-1}+\\gamma_k y^{k}\n\\]\nSoftThreshold operator: \\[ \\operatorname{SoftThreshold}(v, \\tau) = \\operatorname{sign}(v) \\cdot \\max(|v| - \\tau, 0). \\]"}
{"problem_name": "Group Lasso", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda\\sum_{g=1}^G\\|x_{G_g}\\|_2", "class": "Nesterov", "algorithm": "Use Nesterov acceleration with fixed \\(t = 1/L\\), where \\(L = \\|A\\|^2\\). Define:\n\\[\nz^{k}=(1-\\gamma_k)x^{k-1}+\\gamma_k y^{k-1},\n\\]\n\\[\ny^{k}=\\operatorname{GroupShrink}\\left(z^{k}-\\frac{t}{\\gamma_k}A^T(Az^{k}-b), \\frac{t}{\\gamma_k}\\lambda\\right),\n\\]\n\\[\nx^{k}=(1-\\gamma_k)x^{k-1}+\\gamma_k y^{k}\n\\]\nGroupShrink for each group \\(G_g\\): \\[ \\operatorname{GroupShrink}(v_{G_g}, \\tau) = \\left(1 - \\frac{\\tau}{\\|v_{G_g}\\|_2} \\right)_+ v_{G_g}. \\]"}
{"problem_name": "Generalized elastic net", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|B x\\|_2^2", "class": "Nesterov", "algorithm": "Use Nesterov acceleration with fixed \\(t = 1/L\\), where \\(L = 2\\|A\\|^2 + 2\\lambda_2\\|B\\|^2\\). Define:\n\\[\nz^{k}=(1-\\gamma_k)x^{k-1}+\\gamma_k y^{k-1},\n\\]\n\\[\ny^{k}=\\operatorname{SoftThreshold}\\left(z^{k}-\\frac{t}{\\gamma_k}\\nabla f(z^{k}), \\frac{t}{\\gamma_k}\\lambda_1\\right),\n\\]\n\\[\nx^{k}=(1-\\gamma_k)x^{k-1}+\\gamma_k y^{k}\n\\]\nSoftThreshold operator: \\[ \\operatorname{SoftThreshold}(v, \\tau) = \\operatorname{sign}(v) \\cdot \\max(|v| - \\tau, 0). \\]"}
{"problem_name": "Elastic Net regression", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\frac{\\lambda_2}{2}\\|x\\|_2^2", "class": "Nesterov", "algorithm": "Use Nesterov acceleration with fixed \\(t = 1/L\\), where \\(L = \\|A\\|^2 + \\lambda_2\\). Define:\n\\[\nz^{k}=(1-\\gamma_k)x^{k-1}+\\gamma_k y^{k-1},\n\\]\n\\[\ny^{k}=\\operatorname{SoftThreshold}\\left(z^{k}-\\frac{t}{\\gamma_k}\\nabla f(z^{k}), \\frac{t}{\\gamma_k}\\lambda_1\\right),\n\\]\n\\[\nx^{k}=(1-\\gamma_k)x^{k-1}+\\gamma_k y^{k}\n\\]\nSoftThreshold operator: \\[ \\operatorname{SoftThreshold}(v, \\tau) = \\operatorname{sign}(v) \\cdot \\max(|v| - \\tau, 0). \\]"}
{"problem_name": "L2-regularized least squares", "problem_statement": "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|Ax - b\\|^2 + \\mu \\|x\\|_2.", "class": "Nesterov", "algorithm": "We apply Nesterov's accelerated proximal gradient method with fixed step size \\(t = 1/L\\) and momentum \\(\\gamma_k = 2/(k+1)\\), with \\(\\gamma_0 = 1/2\\).\nLet \\(f(x) = \\frac{1}{2} \\|Ax - b\\|^2\\), \\(h(x) = \\mu \\|x\\|_2\\).\nThen, the updates are given by:\n\\[ z^k = (1 - \\gamma_k)x^{k-1} + \\gamma_k y^{k-1}, \\]\n\\[ y^k = \\operatorname{prox}_{(t/\\gamma_k) h}\\left(z^k - \\frac{t}{\\gamma_k} \\nabla f(z^k)\\right), \\]\n\\[ x^k = (1 - \\gamma_k)x^{k-1} + \\gamma_k y^k. \\]\nHere, \\(\\nabla f(x) = A^\\top(Ax - b)\\), and the proximal operator is:\n\\[ \n\\operatorname{prox}_{t h}(x) = \\begin{cases} 0, & \\text{if } \\|x\\|_2 \\leq t \\mu \\\\ \\left(1 - \\frac{t \\mu}{\\|x\\|_2}\\right) x, & \\text{otherwise} \\end{cases}\n\\]"}
{"problem_name": "Joint sparse coding", "problem_statement": "\\min_{x\\in\\mathbb R^n,y\\in\\mathbb R^m}\\frac{1}{2}\\|Ax + B y - b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|y\\|_1", "class": "BCD", "algorithm": "Let \\(H(x,y)=\\frac{1}{2}\\|Ax + B y - b\\|_2^2\\) and define the soft-threshold operator \\(S_\\alpha(u)=\\mathrm{sign}(u)\\odot\\max(|u|-\\alpha,0)\\). For given step sizes \\(c_k,d_k>0\\), update\n\\(x^{k+1}=S_{c_k\\lambda_1}(x^k - c_k\\nabla_x H(x^k,y^k)),\\quad y^{k+1}=S_{d_k\\lambda_2}(y^k - d_k\\nabla_y H(x^{k+1},y^k))\\),\nwhere \\(\\nabla_x H=A^T(Ax + B y - b)\\) and \\(\\nabla_y H=B^T(Ax + B y - b)\\)."}
{"problem_name": "Robust sparse regression", "problem_statement": "\\min_{x\\in\\mathbb R^n,r\\in\\mathbb R^m}\\frac{1}{2}\\|Ax + r - b\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|r\\|_1", "class": "BCD", "algorithm": "Let \\(H(x,r)=\\frac{1}{2}\\|Ax + r - b\\|_2^2\\) and define \\(S_\\alpha(u)=\\mathrm{sign}(u)\\odot\\max(|u|-\\alpha,0)\\). With \\(c_k,d_k>0\\), update\n\\(x^{k+1}=S_{c_k\\lambda_1}(x^k - c_k\\nabla_x H(x^k,r^k)),\\quad r^{k+1}=S_{d_k\\lambda_2}(r^k - d_k\\nabla_r H(x^{k+1},r^k))\\),\nwhere \\(\\nabla_x H=A^T(Ax + r - b)\\), \\(\\nabla_r H=Ax + r - b\\)."}
{"problem_name": "Joint total variation denoising", "problem_statement": "\\min_{x\\in\\mathbb R^n,y\\in\\mathbb R^m} \\frac{1}{2}\\|Ax + B y - b\\|_2^2 + \\lambda_1\\|D x\\|_1 + \\lambda_2\\|E y\\|_1. Assume the rows of D and E are orthonormal, i.e. D D^T = Iₚ, E E^T = I_q", "class": "BCD", "algorithm": "Define  \\[H(x,y)=\\frac{1}{2}\\|Ax + B y - b\\|_2^2,\\]  and the clamp operator \\[\\mathrm{clamp}(z, -1,1)_i=\\max(-1,\\min(1,z_i)).\\]\nFor given step sizes c_k,d_k>0, set\n  \\[u^k=x^k - c_k\\nabla_x H(x^k,y^k),\\quad\n    v^k=y^k - d_k\\nabla_y H(x^{k+1},y^k),\\]\nwith\n  \\[\\nabla_x H=A^T(Ax + B y - b),\\quad\n    \\nabla_y H=B^T(Ax + B y - b).\\]\nThen the proximal‐gradient updates admit the closed form\n  \\[\n  x^{k+1}=u^k - c_k D^T\\bigl(\\mathrm{clamp}(D u^k / (c_k\\lambda_1),-1,1)\\bigr),\\]\n  \\[\n  y^{k+1}=v^k - d_k E^T\\bigl(\\mathrm{clamp}(E v^k / (d_k\\lambda_2),-1,1)\\bigr).\\]"}
{"problem_name": "Elastic net splitting", "problem_statement": "\\min_{x\\in\\mathbb R^n,y\\in\\mathbb R^n}\\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda_1\\|y\\|_1 + \\frac{\\lambda_2}{2}\\|x - y\\|_2^2", "class": "BCD", "algorithm": "Let \\(H(x,y)=\\frac{1}{2}\\|Ax - b\\|_2^2 + \\frac{\\lambda_2}{2}\\|x - y\\|_2^2\\) and \\(S_\\alpha(u)=\\mathrm{sign}(u)\\odot\\max(|u|-\\alpha,0)\\). For \\(c_k,d_k>0\\),\n\\(x^{k+1}=x^k - c_k\\nabla_x H(x^k,y^k),\\quad y^{k+1}=S_{d_k\\lambda_1}(y^k - d_k\\nabla_y H(x^{k+1},y^k))\\),\nwith \\(\\nabla_x H=A^T(Ax - b)+\\lambda_2(x - y)\\), \\(\\nabla_y H=-\\lambda_2(x - y)\\)."}
{"problem_name": "Coupled Logistic Regression", "problem_statement": "\\min_{w\\in\\mathbb R^n,v\\in\\mathbb R^m} \\sum_{i=1}^p\\log\\bigl(1+e^{-b_i(a_i^T w + c_i^T v)}\\bigr) + \\lambda_1\\|w\\|_1 + \\lambda_2\\|v\\|_1", "class": "BCD", "algorithm": "Let \\(H(w,v)=\\sum_{i=1}^p\\log(1+e^{-b_i(a_i^T w + c_i^T v)})\\) and \\(S_\\alpha(u)=\\mathrm{sign}(u)\\odot\\max(|u|-\\alpha,0)\\). For \\(c_k,d_k>0\\),\n\\(w^{k+1}=S_{c_k\\lambda_1}(w^k - c_k\\nabla_w H(w^k,v^k)),\\quad v^{k+1}=S_{d_k\\lambda_2}(v^k - d_k\\nabla_v H(w^{k+1},v^k))\\),\nwhere \\(\\nabla_w H=-\\sum_i\\frac{b_i}{1+e^{b_i(a_i^T w + c_i^T v)}}a_i\\), \\(\\nabla_v H=-\\sum_i\\frac{b_i}{1+e^{b_i(a_i^T w + c_i^T v)}}c_i\\)."}
{"problem_name": "Coupled Elastic Net", "problem_statement": "\\min_{x\\in\\mathbb R^n, y\\in\\mathbb R^n} \\frac{1}{2}\\|D(x + y) - s\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|y\\|_1", "class": "BCD", "algorithm": "Define \\(F(x, y) = \\frac{1}{2}\\|D(x + y) - s\\|_2^2\\). Then the gradients are:\n\\[\\nabla_x F = D^T(D(x + y) - s),\\quad \\nabla_y F = D^T(D(x + y) - s).\\]\nUse soft-thresholding \\(S_\\alpha(\\cdot)\\) and step sizes \\(\\tau_x, \\tau_y\\).\n1) x-update:\n  \\[x^{k+1} = S_{\\tau_x \\lambda_1}\\left(x^k - \\tau_x D^T(D(x^k + y^k) - s)\\right),\\]\n2) y-update:\n  \\[y^{k+1} = S_{\\tau_y \\lambda_2}\\left(y^k - \\tau_y D^T(D(x^{k+1} + y^k) - s)\\right).\\]"}
{"problem_name": "L2 Group Sparse Coding", "problem_statement": "\\min_{x\\in\\mathbb R^n, z\\in\\mathbb R^m} \\frac{1}{2}\\|Ax + Bz - y\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|z\\|_2", "class": "BCD", "algorithm": "Let \\(F(x,z) = \\frac{1}{2}\\|Ax + Bz - y\\|_2^2\\), with gradients:\n\\[\\nabla_x F = A^T(Ax + Bz - y),\\quad \\nabla_z F = B^T(Ax + Bz - y).\\]\nUse soft-thresholding \\(S_\\alpha(\\cdot)\\) and vector shrinkage \\(\\mathrm{Shrink}_2(v, \\alpha) = \\max(1 - \\alpha/\\|v\\|_2, 0)\\cdot v\\).\n1) x-update:\n  \\[x^{k+1} = S_{\\tau_x \\lambda_1}\\left(x^k - \\tau_x A^T(Ax^k + Bz^k - y)\\right),\\]\n2) z-update:\n  \\[z^{k+1} = \\mathrm{Shrink}_2\\left(z^k - \\tau_z B^T(Ax^{k+1} + Bz^k - y),\\tau_z \\lambda_2\\right).\\]"}
{"problem_name": "Coupled Low-Dimensional Projection", "problem_statement": "\\min_{u\\in\\mathbb R^n, v\\in\\mathbb R^n} \\frac{1}{2}\\|Du + v - b\\|_2^2 + \\lambda_1\\|u\\|_2 + \\lambda_2\\|v\\|_1", "class": "BCD", "algorithm": "Let \\(F(u,v) = \\frac{1}{2}\\|Du + v - b\\|_2^2\\), gradients:\n\\[\\nabla_u F = D^T(Du + v - b),\\quad \\nabla_v F = Du + v - b.\\]\nUse vector shrinkage for \\(u\\) and soft-thresholding for \\(v\\).\n1) u-update:\n  \\[u^{k+1} = \\mathrm{Shrink}_2\\left(u^k - \\tau_u D^T(Du^k + v^k - b),\\tau_u \\lambda_1\\right),\\]\n2) v-update:\n  \\[v^{k+1} = S_{\\tau_v \\lambda_2}\\left(v^k - \\tau_v(Du^{k+1} + v^k - b)\\right).\\]"}
{"problem_name": "Residual L2 Decomposition", "problem_statement": "\\min_{x\\in\\mathbb R^n, r\\in\\mathbb R^n} \\frac{1}{2}\\|x + r - y\\|_2^2 + \\lambda_1\\|x\\|_1 + \\lambda_2\\|r\\|_2", "class": "BCD", "algorithm": "Let \\(F(x,r) = \\frac{1}{2}\\|x + r - y\\|_2^2\\), with gradients:\n\\[\\nabla_x F = x + r - y, \\quad \\nabla_r F = x + r - y.\\]\nUse \\(S_\\alpha\\) and L2 shrinkage.\n1) x-update:\n  \\[x^{k+1} = S_{\\tau_x \\lambda_1}\\left(x^k - \\tau_x(x^k + r^k - y)\\right),\\]\n2) r-update:\n  \\[r^{k+1} = \\mathrm{Shrink}_2\\left(r^k - \\tau_r(x^{k+1} + r^k - y),\\tau_r \\lambda_2\\right).\\]"}
{"problem_name": "Constrained Lasso with equality constraint", "problem_statement": "\\min_{x\\in\\mathbb R^n,\\,d\\in\\mathbb R^m}\\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda\\|d\\|_1\\quad\\text{s.t. }C x = d", "class": "ADMM", "algorithm": "Form the augmented Lagrangian:\\n\\[\\nL_\\rho(x,d,\\mu)=\\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda\\|d\\|_1 + \\mu^T(Cx - d) + \\frac{\\rho}{2}\\|Cx - d\\|_2^2,\\n\\]\\nwhere \\(\\mu\\in\\mathbb{R}^m\\) is the dual variable and \\(\\rho>0\\) is the penalty parameter. At each iteration \\(k = 0,1,2,...\\), perform the following updates:\\n1. x-update (quadratic minimization):\\n\\[\\nx^{k+1} = (A^\\top A + \\rho C^\\top C)^{-1}(A^\\top b + \\rho C^\\top(d^k - \\mu^k / \\rho))\\n\\]\\n2. d-update (proximal step of \\(\\ell_1\\)-norm):\\n\\[\\nd^{k+1} = \\operatorname{SoftThreshold}(Cx^{k+1} + \\mu^k / \\rho, \\lambda / \\rho)\\n\\]\\n3. Dual variable update:\\n\\[\\n\\mu^{k+1} = \\mu^k + \\tau \\rho (C x^{k+1} - d^{k+1})\\n\\]\\nThe soft-thresholding operator is defined elementwise by:\\n\\[\\n\\operatorname{SoftThreshold}(v, \\tau) = \\operatorname{sign}(v) \\cdot \\max(|v| - \\tau, 0)\\n\\]"}
{"problem_name": "TV denoising with constraint splitting", "problem_statement": " \\min_{x,z\\in\\mathbb R^n}\\frac{1}{2}\\|x - b\\|_2^2 + \\lambda\\|z\\|_1\\quad\\text{s.t.}z = D x", "class": "ADMM", "algorithm": "Form the augmented Lagrangian\n  \\[L_\\rho(x,z,\\nu)=\\frac{1}{2}\\|x - b\\|_2^2 + \\lambda\\|z\\|_1 + \\nu^T(z - D x) + \\frac{\\rho}{2}\\|z - D x\\|_2^2.\\]\nThen iterate:\n  1) x-update (linear solve)\n  \\[x^{k+1}=(I + \\rho D^T D)^{-1}\\bigl(b + D^T(\\rho z^k - \\nu^k)\\bigr),\\]\n  2) z-update (soft-threshold)\n  \\[z^{k+1}_i=\\mathrm{sign}\\bigl([D x^{k+1} + \\frac{\\nu^k}{\\rho}]_i\\bigr)\\max\\bigl(|[D x^{k+1} + \\frac{\\nu^k}{\\rho}]_i| - \\frac{\\lambda}{\\rho},0\\bigr),\\]\n  3) dual update\n  \\[\\nu^{k+1}=\\nu^k + \\tau \\rho\\bigl(z^{k+1} - D x^{k+1}\\bigr).\\]"}
{"problem_name": "Lasso problem with variable splitting", "problem_statement": " \\min_{x,y\\in\\mathbb R^n}\\frac{1}{2}\\|Ax - b\\|_2^2 + \\mu\\|y\\|_1\\quad\\text{s.t.}x = y", "class": "ADMM", "algorithm": "Form the augmented Lagrangian\n  \\[L_\\rho(x,y,\\nu)=\\frac{1}{2}\\|Ax - b\\|_2^2 + \\mu\\|y\\|_1 + \\nu^T(x - y) + \\frac{\\rho}{2}\\|x - y\\|_2^2.\\]\nThen iterate:\n  1) x-update (linear solve)\n  \\[x^{k+1}=(A^T A + \\rho I)^{-1}\\bigl(A^T b + \\rho(y^k - \\frac{\\nu^k}{\\rho})\\bigr),\\]\n  2) y-update (soft-threshold)\n  \\[y^{k+1}_i=\\mathrm{sign}\\bigl(x^{k+1}_i + \\frac{\\nu^k_i}{\\rho}\\bigr)\\max\\bigl(|x^{k+1}_i + \\frac{\\nu^k_i}{\\rho}| - \\frac{\\mu}{\\rho},0\\bigr),\\]\n  3) dual update\n  \\[\\nu^{k+1}=\\nu^k + \\tau \\rho\\bigl(x^{k+1} - y^{k+1}\\bigr).\\]"}
{"problem_name": "Group Lasso with Overlapping Groups", "problem_statement": "\\min_{x\\in\\mathbb R^n, z\\in\\mathbb R^n} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\sum_{g\\in G} \\|z_g\\|_2 \\quad \\text{s.t. } x = z", "class": "ADMM", "algorithm": "Form the augmented Lagrangian:\n  \\[L_\\rho(x,z,\\nu) = \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\sum_{g\\in G} \\|z_g\\|_2 + \\nu^T(x - z) + \\frac{\\rho}{2}\\|x - z\\|_2^2.\\]\nThen iterate:\n  1) x-update (linear solve)\n  \\[x^{k+1} = (A^TA + \\rho I)^{-1}(A^Tb + \\rho(z^k - \\nu^k/\\rho)),\\]\n  2) z-update (group soft-thresholding)\n  \\[z^{k+1}_g = \\max\\left(1 - \\frac{\\lambda}{\\rho\\|x^{k+1}_g + \\nu^k_g/\\rho\\|_2}, 0\\right)(x^{k+1}_g + \\nu^k_g/\\rho),\\]\n  3) dual update\n  \\[\\nu^{k+1} = \\nu^k + \\tau \\rho(x^{k+1} - z^{k+1}).\\]"}
{"problem_name": "Elastic Net regression", "problem_statement": "\\min_{x, z \\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda_1\\|z\\|_1 + \\frac{\\lambda_2}{2}\\|x\\|_2^2 \\quad \\text{s.t. } x = z", "class": "ADMM", "algorithm": "Augmented Lagrangian:\n  \\[L_\\rho(x,z,\\nu) = \\frac{1}{2}\\|Ax - b\\|_2^2 + \\frac{\\lambda_2}{2}\\|x\\|_2^2 + \\lambda_1\\|z\\|_1 + \\nu^T(x - z) + \\frac{\\rho}{2}\\|x - z\\|_2^2.\\]\nIteration:\n  1) x-update:\n  \\[x^{k+1} = (A^T A + (\\lambda_2 + \\rho) I)^{-1}(A^T b + \\rho z^k - \\nu^k),\\]\n  2) z-update (soft-thresholding):\n  \\[z^{k+1}_i = \\mathrm{sign}(u_i)\\cdot\\max(|u_i| - \\lambda_1/\\rho, 0),\\quad u = x^{k+1} + \\nu^k/\\rho,\\]\n  3) dual update:\n  \\[\\nu^{k+1} = \\nu^k + \\tau \\rho(x^{k+1} - z^{k+1}).\\]"}
{"problem_name": "Ridge regression with variable splitting", "problem_statement": "\\min_{x, z \\in \\mathbb{R}^n} \\frac{1}{2}\\|x - c\\|_2^2 + \\frac{\\lambda}{2}\\|z\\|_2^2 \\quad \\text{s.t. } x = z", "class": "ADMM", "algorithm": "Form the augmented Lagrangian:\n  \\[L_\\rho(x,z,\\nu) = \\frac{1}{2}\\|x - c\\|_2^2 + \\frac{\\lambda}{2}\\|z\\|_2^2 + \\nu^T(x - z) + \\frac{\\rho}{2}\\|x - z\\|_2^2.\\]\nThen iterate:\n  1) x-update (closed-form):\n  \\[x^{k+1} = \\frac{c + \\rho z^k - \\nu^k}{1 + \\rho},\\]\n  2) z-update (closed-form):\n  \\[z^{k+1} = \\frac{\\rho x^{k+1} + \\nu^k}{\\lambda + \\rho},\\]\n  3) dual update:\n  \\[\\nu^{k+1} = \\nu^k + \\tau \\rho(x^{k+1} - z^{k+1}).\\]"}
{"problem_name": "Quadratic objective with sparse equality constraint", "problem_statement": "\\min_{x \\in \\mathbb{R}^n,\\ d \\in \\mathbb{R}^m} \\frac{1}{2}\\|x - c\\|_2^2 + \\lambda\\|d\\|_1 \\quad \\text{s.t. } Bx = d", "class": "ADMM", "algorithm": "Form the augmented Lagrangian:\n  \\[L_\\rho(x,d,\\nu) = \\frac{1}{2}\\|x - c\\|_2^2 + \\lambda\\|d\\|_1 + \\nu^T(Bx - d) + \\frac{\\rho}{2}\\|Bx - d\\|_2^2.\\]\nThen iterate:\n  1) x-update (linear solve):\n  \\[x^{k+1} = (I + \\rho B^T B)^{-1}(c + \\rho B^T(d^k - \\nu^k/\\rho)),\\]\n  2) d-update (soft-threshold):\n  \\[d^{k+1}_i = \\mathrm{sign}(u_i)\\cdot\\max(|u_i| - \\lambda/\\rho, 0),\\quad u = Bx^{k+1} + \\nu^k/\\rho,\\]\n  3) dual update:\n  \\[\\nu^{k+1} = \\nu^k + \\tau \\rho(Bx^{k+1} - d^{k+1}).\\]"}
